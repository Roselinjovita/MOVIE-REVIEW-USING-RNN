{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvtM7iHp0koY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu5R6gSn07Z4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUfG35yd08qL",
        "outputId": "6e35de02-257a-4693-ac35-b3b6378babc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASm7KZA-0_0-"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "train_df = pd.read_csv('/content/twitter_training.csv', header=None, names=['id', 'entity', 'category', 'clean_text'])\n",
        "val_df = pd.read_csv('/content/twitter_validation.csv', header=None, names=['id', 'entity', 'category', 'clean_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TVxnDvN1T5q"
      },
      "outputs": [],
      "source": [
        "# Combine and filter\n",
        "df = pd.concat([train_df, val_df])\n",
        "df = df[['clean_text', 'category']].dropna()\n",
        "df = df[df['category'].isin(['Positive', 'Neutral', 'Negative'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtggYIoZ1Y-O"
      },
      "outputs": [],
      "source": [
        "# Label encoding\n",
        "label_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "df['label'] = df['category'].map(label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww-XylED38YW",
        "outputId": "9f7d2aaa-fa30-4e01-dc7b-8a13a708c41c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Text Preprocessing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) - {\"not\", \"no\", \"nor\"}\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CM-AUTH38U7"
      },
      "outputs": [],
      "source": [
        "df['clean_text'] = df['clean_text'].astype(str).apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCixTsxQ38Sp"
      },
      "outputs": [],
      "source": [
        "# Tokenization and padding\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['clean_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['clean_text'])\n",
        "X = pad_sequences(sequences, maxlen=60, padding='post', truncating='post')\n",
        "y = to_categorical(df['label'], num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U54pl6KZ38QS"
      },
      "outputs": [],
      "source": [
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXukcRFL38OP",
        "outputId": "b7793125-75ad-4f0b-bfd7-cb643073b7a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Build RNN model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=64, input_length=60),\n",
        "    SimpleRNN(64),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-baPNE-738Lh"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q91YrFOz38JC",
        "outputId": "bc85cd24-80c4-4402-d071-490614ea1ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 18ms/step - accuracy: 0.3606 - loss: 1.1007 - val_accuracy: 0.3644 - val_loss: 1.0955\n",
            "Epoch 2/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 19ms/step - accuracy: 0.3807 - loss: 1.1061 - val_accuracy: 0.3660 - val_loss: 1.0992\n",
            "Epoch 3/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 19ms/step - accuracy: 0.3519 - loss: 1.1043 - val_accuracy: 0.3646 - val_loss: 1.0973\n",
            "Epoch 4/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 18ms/step - accuracy: 0.3626 - loss: 1.0992 - val_accuracy: 0.3646 - val_loss: 1.1116\n",
            "Epoch 5/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 19ms/step - accuracy: 0.3464 - loss: 1.1028 - val_accuracy: 0.3102 - val_loss: 1.1116\n",
            "Epoch 6/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 19ms/step - accuracy: 0.3551 - loss: 1.1030 - val_accuracy: 0.3763 - val_loss: 1.0934\n",
            "Epoch 7/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 18ms/step - accuracy: 0.3611 - loss: 1.1013 - val_accuracy: 0.3648 - val_loss: 1.0945\n",
            "Epoch 8/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 18ms/step - accuracy: 0.3442 - loss: 1.1042 - val_accuracy: 0.3646 - val_loss: 1.0950\n",
            "Epoch 9/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 19ms/step - accuracy: 0.3474 - loss: 1.1031 - val_accuracy: 0.3646 - val_loss: 1.0979\n",
            "Epoch 10/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 19ms/step - accuracy: 0.3533 - loss: 1.1027 - val_accuracy: 0.3514 - val_loss: 1.0951\n",
            "Epoch 11/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 18ms/step - accuracy: 0.3508 - loss: 1.1026 - val_accuracy: 0.3802 - val_loss: 1.0949\n",
            "Epoch 12/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 18ms/step - accuracy: 0.3529 - loss: 1.1033 - val_accuracy: 0.3418 - val_loss: 1.1047\n",
            "Epoch 13/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 18ms/step - accuracy: 0.3468 - loss: 1.1054 - val_accuracy: 0.3518 - val_loss: 1.0965\n",
            "Epoch 14/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 18ms/step - accuracy: 0.3551 - loss: 1.1027 - val_accuracy: 0.3646 - val_loss: 1.0956\n",
            "Epoch 15/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 18ms/step - accuracy: 0.3485 - loss: 1.1035 - val_accuracy: 0.3530 - val_loss: 1.0959\n",
            "Epoch 16/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 19ms/step - accuracy: 0.3482 - loss: 1.1051 - val_accuracy: 0.3646 - val_loss: 1.0981\n",
            "Epoch 17/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 19ms/step - accuracy: 0.3538 - loss: 1.1034 - val_accuracy: 0.3536 - val_loss: 1.0959\n",
            "Epoch 18/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 19ms/step - accuracy: 0.3498 - loss: 1.1036 - val_accuracy: 0.3560 - val_loss: 1.0971\n",
            "Epoch 19/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 19ms/step - accuracy: 0.3501 - loss: 1.1041 - val_accuracy: 0.3646 - val_loss: 1.0982\n",
            "Epoch 20/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 19ms/step - accuracy: 0.3533 - loss: 1.1030 - val_accuracy: 0.3646 - val_loss: 1.0956\n",
            "Epoch 21/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 19ms/step - accuracy: 0.3564 - loss: 1.1020 - val_accuracy: 0.3210 - val_loss: 1.1027\n",
            "Epoch 22/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 18ms/step - accuracy: 0.3423 - loss: 1.1045 - val_accuracy: 0.3599 - val_loss: 1.0988\n",
            "Epoch 23/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 18ms/step - accuracy: 0.3514 - loss: 1.1037 - val_accuracy: 0.3646 - val_loss: 1.0949\n",
            "Epoch 24/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 19ms/step - accuracy: 0.3500 - loss: 1.1047 - val_accuracy: 0.3420 - val_loss: 1.0955\n",
            "Epoch 25/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 18ms/step - accuracy: 0.3496 - loss: 1.1039 - val_accuracy: 0.3646 - val_loss: 1.0999\n",
            "Epoch 26/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 19ms/step - accuracy: 0.3438 - loss: 1.1044 - val_accuracy: 0.3646 - val_loss: 1.0949\n",
            "Epoch 27/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 18ms/step - accuracy: 0.3448 - loss: 1.1042 - val_accuracy: 0.3646 - val_loss: 1.0956\n",
            "Epoch 28/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 19ms/step - accuracy: 0.3456 - loss: 1.1053 - val_accuracy: 0.3646 - val_loss: 1.0957\n",
            "Epoch 29/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 19ms/step - accuracy: 0.3472 - loss: 1.1047 - val_accuracy: 0.3420 - val_loss: 1.0959\n",
            "Epoch 30/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 18ms/step - accuracy: 0.3464 - loss: 1.1031 - val_accuracy: 0.3646 - val_loss: 1.0961\n",
            "Epoch 31/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 18ms/step - accuracy: 0.3491 - loss: 1.1042 - val_accuracy: 0.3646 - val_loss: 1.0967\n",
            "Epoch 32/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 19ms/step - accuracy: 0.3444 - loss: 1.1058 - val_accuracy: 0.3601 - val_loss: 1.0977\n",
            "Epoch 33/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 19ms/step - accuracy: 0.3500 - loss: 1.1042 - val_accuracy: 0.3592 - val_loss: 1.0994\n",
            "Epoch 34/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 18ms/step - accuracy: 0.3479 - loss: 1.1043 - val_accuracy: 0.3420 - val_loss: 1.0962\n",
            "Epoch 35/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 19ms/step - accuracy: 0.3518 - loss: 1.1043 - val_accuracy: 0.3646 - val_loss: 1.0963\n",
            "Epoch 36/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 19ms/step - accuracy: 0.3514 - loss: 1.1033 - val_accuracy: 0.3646 - val_loss: 1.1012\n",
            "Epoch 37/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 19ms/step - accuracy: 0.3477 - loss: 1.1047 - val_accuracy: 0.3420 - val_loss: 1.0968\n",
            "Epoch 38/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 19ms/step - accuracy: 0.3495 - loss: 1.1042 - val_accuracy: 0.3420 - val_loss: 1.1023\n",
            "Epoch 39/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 18ms/step - accuracy: 0.3489 - loss: 1.1042 - val_accuracy: 0.3646 - val_loss: 1.0974\n",
            "Epoch 40/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 19ms/step - accuracy: 0.3493 - loss: 1.1048 - val_accuracy: 0.3646 - val_loss: 1.1140\n",
            "Epoch 41/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 19ms/step - accuracy: 0.3433 - loss: 1.1044 - val_accuracy: 0.3646 - val_loss: 1.0996\n",
            "Epoch 42/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 18ms/step - accuracy: 0.3471 - loss: 1.1043 - val_accuracy: 0.3646 - val_loss: 1.1012\n",
            "Epoch 43/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 18ms/step - accuracy: 0.3493 - loss: 1.1046 - val_accuracy: 0.3646 - val_loss: 1.0962\n",
            "Epoch 44/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 19ms/step - accuracy: 0.3486 - loss: 1.1050 - val_accuracy: 0.2933 - val_loss: 1.1028\n",
            "Epoch 45/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 18ms/step - accuracy: 0.3500 - loss: 1.1047 - val_accuracy: 0.3420 - val_loss: 1.0971\n",
            "Epoch 46/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 19ms/step - accuracy: 0.3492 - loss: 1.1031 - val_accuracy: 0.3472 - val_loss: 1.0964\n",
            "Epoch 47/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 19ms/step - accuracy: 0.3473 - loss: 1.1042 - val_accuracy: 0.3471 - val_loss: 1.1010\n",
            "Epoch 48/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 19ms/step - accuracy: 0.3520 - loss: 1.1036 - val_accuracy: 0.3582 - val_loss: 1.0979\n",
            "Epoch 49/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 18ms/step - accuracy: 0.3506 - loss: 1.1049 - val_accuracy: 0.3647 - val_loss: 1.1003\n",
            "Epoch 50/50\n",
            "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 19ms/step - accuracy: 0.3462 - loss: 1.1049 - val_accuracy: 0.3446 - val_loss: 1.1037\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x782b45fd9890>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=10, validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1-3ACuSe38GG"
      },
      "outputs": [],
      "source": [
        "# Prediction function\n",
        "def predict_sentiment(text):\n",
        "    text = preprocess(text)\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(seq, maxlen=60, padding='post')\n",
        "    pred = model.predict(padded)\n",
        "    label = np.argmax(pred)\n",
        "    return {0: 'Negative', 1: 'Neutral', 2: 'Positive'}[label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FxtKgIta38D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b14ac8-08b3-43bb-dcaa-312fbb810bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a tweet (or type 'exit'): bad movie\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step\n",
            "Predicted Sentiment: Negative\n",
            "Enter a tweet (or type 'exit'): spider man movie was mind blowing as always\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "Predicted Sentiment: Neutral\n",
            "Enter a tweet (or type 'exit'): conjuring movie was amazing\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "Predicted Sentiment: Negative\n",
            "Enter a tweet (or type 'exit'): exit\n"
          ]
        }
      ],
      "source": [
        "# Interactive input\n",
        "while True:\n",
        "    user_input = input(\"Enter a tweet (or type 'exit'): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    sentiment = predict_sentiment(user_input)\n",
        "    print(f\"Predicted Sentiment: {sentiment}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pmvD0wb638BW"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit pyngrok --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cPkMwWj637-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a295fd95-caf0-455d-c010-6460e52260ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Save model\n",
        "model.save('sentiment_rnn_model.h5')\n",
        "\n",
        "# Save tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "LMhp8AR8378R"
      },
      "outputs": [],
      "source": [
        "with open('label_map.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_map, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5hKMMDF23750"
      },
      "outputs": [],
      "source": [
        "app_code = \"\"\"\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load trained model and tokenizer\n",
        "model = tf.keras.models.load_model('sentiment_rnn_model.h5')\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Try loading label encoder\n",
        "try:\n",
        "    with open('label_encoder.pickle', 'rb') as handle:\n",
        "        label_encoder = pickle.load(handle)\n",
        "    use_label_encoder = True\n",
        "except:\n",
        "    use_label_encoder = False\n",
        "\n",
        "# Set max sequence length (same as training)\n",
        "MAX_LEN = 200\n",
        "\n",
        "st.title(\"Sentiment Analysis with RNN\")\n",
        "st.subheader(\"Enter text below to get sentiment prediction:\")\n",
        "\n",
        "user_input = st.text_area(\"Your Input:\")\n",
        "\n",
        "if st.button(\"Predict Sentiment\"):\n",
        "    if user_input.strip() == \"\":\n",
        "        st.warning(\"Please enter some text.\")\n",
        "    else:\n",
        "        sequence = tokenizer.texts_to_sequences([user_input])\n",
        "        padded = pad_sequences(sequence, maxlen=MAX_LEN)\n",
        "        prediction = model.predict(padded)[0]\n",
        "        sentiment_class = prediction.argmax()\n",
        "\n",
        "        if use_label_encoder:\n",
        "            sentiment = label_encoder.inverse_transform([sentiment_class])[0]\n",
        "        else:\n",
        "            sentiment = [\"Negative\", \"Neutral\", \"Positive\"][sentiment_class]\n",
        "\n",
        "        st.success(f\"Predicted Sentiment: {sentiment}\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "XT6NM3wH373U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42fb8ee-30e2-4b3e-8b92-c56a2a5eb1c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New tunnel created: NgrokTunnel: \"https://5cc4a97e7469.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/dev/null &\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"30d7b5xfBENTlBOsOoBtNpORBGs_4siWMU9NYyjDTvXBBBiPf\")\n",
        "\n",
        "\n",
        "# Disconnect all existing tunnels\n",
        "for tunnel in ngrok.get_tunnels():\n",
        "    ngrok.disconnect(tunnel.public_url)\n",
        "    print(f\"Disconnected tunnel: {tunnel.public_url}\")\n",
        "\n",
        "# Connect to a new tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"New tunnel created: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DruplkSiTRJ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}